# Tokenizers

This folder contains implementations of tokenization techniques for natural language processing (NLP).

---

## ğŸ“š Purpose

Tokenization is a crucial preprocessing step that breaks raw data into tokens that models can understand and process.  
These implementations aim to provide clear, from-scratch versions of popular tokenizers to deepen understanding of their inner workings.

---

## ğŸš€ Current Implementations

- **NLP Tokenizer** 
- **Vision Tokenizer**
  
---

## ğŸ—‚ï¸ Folder Structure

```
tokenizers/
â””â”€â”€ nlp/
    â”œâ”€â”€ word_level/
    â””â”€â”€ bpe/
```
