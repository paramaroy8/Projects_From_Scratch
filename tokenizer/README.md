# Tokenizers

This folder contains implementations of tokenization techniques for natural language processing (NLP).

---

## 📚 Purpose

Tokenization is a crucial preprocessing step that breaks raw text into tokens that models can understand and process.  
These implementations aim to provide clear, from-scratch versions of popular tokenizers to deepen understanding of their inner workings.

---

## 🚀 Current Implementations

- **Word-Level Tokenizer**  
  A simple tokenizer that splits text based on whitespace and punctuation to produce word tokens.

- **Byte Pair Encoding (BPE) Tokenizer**  
  An implementation of the BPE algorithm that iteratively merges frequent character pairs to create subword units, improving vocabulary efficiency.

---

## 🗂️ Folder Structure

```
tokenizers/
└── nlp/
    ├── word_level/
    └── bpe/
```
