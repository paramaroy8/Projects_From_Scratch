# Tokenizers

This folder contains implementations of tokenization techniques for natural language processing (NLP).

---

## ğŸ“š Purpose

Tokenization is a crucial preprocessing step that breaks raw text into tokens that models can understand and process.  
These implementations aim to provide clear, from-scratch versions of popular tokenizers to deepen understanding of their inner workings.

---

## ğŸš€ Current Implementations

- **Word-Level Tokenizer**  
  A simple tokenizer that splits text based on whitespace and punctuation to produce word tokens.

- **Byte Pair Encoding (BPE) Tokenizer**  
  An implementation of the BPE algorithm that iteratively merges frequent character pairs to create subword units, improving vocabulary efficiency.

---

## ğŸ—‚ï¸ Folder Structure

```
tokenizers/
â””â”€â”€ nlp/
    â”œâ”€â”€ word_level/
    â””â”€â”€ bpe/
```
