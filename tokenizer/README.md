# Tokenizers

This folder contains implementations of tokenization techniques for natural language processing (NLP).

---

## 📚 Purpose

Tokenization is a crucial preprocessing step that breaks raw data into tokens that models can understand and process.  
These implementations aim to provide clear, from-scratch versions of popular tokenizers to deepen understanding of their inner workings.

---

## 🚀 Current Implementations

- **NLP Tokenizer** 
- **Vision Tokenizer**
  
---

## 🗂️ Folder Structure

```
tokenizers/
└── nlp/
    ├── word_level/
    └── bpe/
```
